"""
setup.py
Script d'installation et de configuration initiale
"""

import os
import sys
import subprocess
from pathlib import Path
import shutil


def print_header(text):
    """Affiche un header stylis√©"""
    print("\n" + "="*60)
    print(f"  {text}")
    print("="*60 + "\n")


def check_python_version():
    """V√©rifie la version de Python"""
    print("üîç V√©rification de Python...")
    
    if sys.version_info < (3, 9):
        print("‚ùå Python 3.9+ requis!")
        print(f"   Version actuelle: {sys.version}")
        return False
    
    print(f"‚úÖ Python {sys.version_info.major}.{sys.version_info.minor} d√©tect√©")
    return True


def check_ollama():
    """V√©rifie si Ollama est install√©"""
    print("\nüîç V√©rification d'Ollama...")
    
    try:
        result = subprocess.run(
            ["ollama", "--version"],
            capture_output=True,
            text=True,
            timeout=5
        )
        
        if result.returncode == 0:
            print("‚úÖ Ollama install√©")
            
            # Lister les mod√®les
            try:
                models_result = subprocess.run(
                    ["ollama", "list"],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                
                if models_result.returncode == 0:
                    lines = models_result.stdout.strip().split('\n')
                    if len(lines) > 1:
                        print(f"   Mod√®les disponibles: {len(lines)-1}")
                    else:
                        print("‚ö†Ô∏è  Aucun mod√®le t√©l√©charg√©")
                        print("   Ex√©cute: ollama pull mistral-nemo")
            except Exception:
                pass
            
            return True
        else:
            print("‚ùå Ollama non install√© ou non accessible")
            print("   T√©l√©charge depuis: https://ollama.ai")
            return False
    
    except FileNotFoundError:
        print("‚ùå Ollama non install√©")
        print("   T√©l√©charge depuis: https://ollama.ai")
        return False
    
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur lors de la v√©rification: {e}")
        return False


def create_directory_structure(base_dir):
    """Cr√©e la structure de r√©pertoires"""
    print("\nüìÅ Cr√©ation de la structure de r√©pertoires...")
    
    base_path = Path(base_dir).expanduser()
    
    dirs = [
        base_path,
        base_path / "Data",
        base_path / "Characters",
        base_path / "lames_db",
        base_path / "saved_sessions",
        base_path / "memory"
    ]
    
    for directory in dirs:
        directory.mkdir(parents=True, exist_ok=True)
        print(f"   ‚úÖ {directory}")
    
    # Cr√©er des fichiers README dans les dossiers importants
    readme_data = base_path / "Data" / "README.txt"
    if not readme_data.exists():
        readme_data.write_text(
            "D√©pose ici tes PDFs et documents de r√®gles du jeu.\n"
            "Formats support√©s: .pdf, .txt, .md\n"
        )
    
    readme_chars = base_path / "Characters" / "README.txt"
    if not readme_chars.exists():
        readme_chars.write_text(
            "D√©pose ici les fiches de personnages.\n"
            "Formats support√©s: .pdf, .txt, .md\n"
        )
    
    return base_path


def create_config_file(base_dir):
    """Cr√©e le fichier de configuration"""
    print("\n‚öôÔ∏è  Cr√©ation du fichier config.yaml...")
    
    config_path = Path("config.yaml")
    
    if config_path.exists():
        response = input("   config.yaml existe d√©j√†. √âcraser? (o/N): ")
        if response.lower() != 'o':
            print("   ‚è≠Ô∏è  Conservation du fichier existant")
            return
    
    config_content = f"""# Configuration - Assistant MJ Les Lames du Cardinal

# Chemins (utilise ~ pour home directory, ou chemins absolus)
paths:
  base_dir: "{base_dir}"
  pdf_root: "Data"
  char_dir: "Characters"
  db_dir: "lames_db"
  save_dir: "saved_sessions"
  memory_dir: "memory"

# Configuration du mod√®le
model:
  default: "mistral-nemo"
  temperature: 0.0
  top_p: 1.0
  fallback_models:
    - "llama3"
    - "mistral"
    - "phi3"

# Configuration RAG
rag:
  embedding_model: "all-MiniLM-L6-v2"
  k_retrieval: 6
  chunk_size: 1000
  chunk_overlap: 150
  use_cuda: true

# Configuration de la m√©moire
memory:
  max_mj_memory: 30
  max_encyclo_memory: 10
  short_memory_context: 3

# Configuration UI
ui:
  default_mode: "MJ immersif"
  show_sources_default: false
  timeline_enabled: true
  auto_save_interval: 5

# Prompts
prompts:
  mj_system: |
    Tu es le Ma√Ætre de Jeu des Lames du Cardinal.
    Tu dois r√©pondre **uniquement** √† partir des extraits fournis ci-dessous.
    Si l'information n'existe pas dans le contexte, r√©ponds exactement : "Aucune information trouv√©e dans le corpus."
    N'invente JAMAIS d'informations.
    
  encyclo_system: |
    Tu es une encyclop√©die du jeu 'Les Lames du Cardinal'.
    R√©ponds uniquement selon le contenu des extraits fournis.
    Si aucune information n'est trouv√©e, r√©ponds exactement : "Aucune information trouv√©e dans le corpus."
    Ne cr√©e jamais d'informations de toutes pi√®ces.

# Options avanc√©es
advanced:
  max_pdf_pages: 500
  enable_statistics: true
  export_format: "markdown"
"""
    
    config_path.write_text(config_content)
    print(f"   ‚úÖ config.yaml cr√©√©")


def install_dependencies():
    """Installe les d√©pendances"""
    print("\nüì¶ Installation des d√©pendances...")
    
    requirements_path = Path("requirements.txt")
    
    if not requirements_path.exists():
        print("   ‚ùå requirements.txt non trouv√©!")
        return False
    
    print("   Cela peut prendre quelques minutes...")
    
    try:
        subprocess.run(
            [sys.executable, "-m", "pip", "install", "-r", "requirements.txt"],
            check=True
        )
        print("   ‚úÖ D√©pendances install√©es")
        return True
    
    except subprocess.CalledProcessError as e:
        print(f"   ‚ùå Erreur lors de l'installation: {e}")
        return False


def download_ollama_model(model_name="mistral-nemo"):
    """Propose de t√©l√©charger un mod√®le Ollama"""
    print(f"\nü§ñ T√©l√©chargement du mod√®le {model_name}...")
    
    response = input(f"   T√©l√©charger {model_name}? (o/N): ")
    
    if response.lower() != 'o':
        print("   ‚è≠Ô∏è  T√©l√©chargement annul√©")
        return False
    
    try:
        print(f"   T√©l√©chargement en cours (cela peut prendre plusieurs minutes)...")
        subprocess.run(
            ["ollama", "pull", model_name],
            check=True
        )
        print(f"   ‚úÖ Mod√®le {model_name} t√©l√©charg√©")
        return True
    
    except subprocess.CalledProcessError as e:
        print(f"   ‚ùå Erreur lors du t√©l√©chargement: {e}")
        return False
    
    except FileNotFoundError:
        print("   ‚ùå Ollama non trouv√©")
        return False


def main():
    """Fonction principale"""
    print_header("üó°Ô∏è  INSTALLATION - ASSISTANT MJ LES LAMES DU CARDINAL")
    
    # V√©rifications
    if not check_python_version():
        return
    
    has_ollama = check_ollama()
    
    # Demander le r√©pertoire de base
    print("\nüìÇ Configuration du r√©pertoire de base")
    default_dir = str(Path.home() / "LamesMJ")
    base_dir = input(f"   R√©pertoire de base [{default_dir}]: ").strip()
    
    if not base_dir:
        base_dir = default_dir
    
    # Cr√©er la structure
    base_path = create_directory_structure(base_dir)
    
    # Cr√©er le fichier de config
    create_config_file(base_dir)
    
    # Installer les d√©pendances
    print("\nüì¶ Installation des d√©pendances")
    install_deps = input("   Installer les d√©pendances Python? (O/n): ").strip()
    
    if install_deps.lower() != 'n':
        install_dependencies()
    
    # T√©l√©charger un mod√®le Ollama
    if has_ollama:
        download_ollama_model("mistral-nemo")
    
    # R√©sum√©
    print_header("‚úÖ INSTALLATION TERMIN√âE")
    
    print("üìã Prochaines √©tapes:")
    print(f"   1. Ajoute tes PDFs de r√®gles dans: {base_path / 'Data'}")
    print(f"   2. Ajoute tes fiches de personnages dans: {base_path / 'Characters'}")
    print("   3. Lance l'application avec: streamlit run app.py")
    
    if not has_ollama:
        print("\n‚ö†Ô∏è  N'oublie pas d'installer Ollama: https://ollama.ai")
    
    print("\nüéÆ Bon jeu! üó°Ô∏è")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Installation annul√©e")
    except Exception as e:
        print(f"\n\n‚ùå Erreur inattendue: {e}")
        import traceback
        traceback.print_exc()